# 百果园舆情系统 - 需求分析与升级方案

## 📋 你的需求清单

1. **定时爬虫**: 每天3次自动爬取全网舆情新闻
2. **数据保留**: 新闻存储14天即可
3. **多人协作**: 网站链接可以分享给其他人一同查看和操作
4. **生产环境**: 部署在云服务器,稳定可靠

---

## ✅ 当前系统功能评估

### 1. 定时爬虫功能 ✅ **完全满足**

**现状**:
- ✅ 已有 `scheduler_daemon.py` 定时任务程序
- ✅ 默认配置每天3次: 早8点、中午13点、晚8点
- ✅ 支持4大平台: 小红书、抖音、微博、B站
- ✅ 关键词已配置: "百果园&贵"、"百果园&刺客"等

**代码位置**: `scheduler_daemon.py` (第145-147行)
```python
schedule.every().day.at("08:00").do(morning_task)
schedule.every().day.at("13:00").do(noon_task)
schedule.every().day.at("20:00").do(evening_task)
```

### 2. 数据保留14天 ⚠️ **需要添加**

**现状**:
- ❌ 当前没有自动清理历史数据的功能
- ✅ 数据库是SQLite,可以轻松添加定时清理

**需要做什么**: 添加一个定时任务,每天凌晨删除14天前的数据

### 3. 多人协作访问 ⚠️ **SQLite有限制**

**现状**:
- ✅ Web界面支持多人同时访问(Streamlit)
- ⚠️ SQLite数据库不支持高并发写入
- ⚠️ 多人同时标记"已处理"可能会冲突

**问题分析**:
| 场景 | SQLite表现 | MySQL/PostgreSQL表现 |
|------|-----------|---------------------|
| 多人查看数据 | ✅ 正常 | ✅ 正常 |
| 1-2人操作 | ✅ 正常 | ✅ 正常 |
| 3-5人同时操作 | ⚠️ 可能卡顿 | ✅ 流畅 |
| 10+人同时操作 | ❌ 会出错 | ✅ 流畅 |

### 4. 云端部署 ✅ **已准备好**

**现状**:
- ✅ 已有服务器: 101.201.214.42
- ✅ 已有一键部署脚本
- ✅ 支持后台运行

---

## 🎯 推荐方案:渐进式升级

### 方案A: 快速上线版(1小时) ⭐ **推荐先用这个**

**适用场景**: 2-3人使用,快速验证需求

**优点**:
- ✅ 无需购买云数据库(省钱)
- ✅ 部署简单,1小时搞定
- ✅ 满足基本需求

**缺点**:
- ⚠️ 不支持10人以上同时操作
- ⚠️ 数据备份需要手动

**需要做的改动**:
1. ✅ 使用SQLite数据库(已有)
2. ✅ 部署定时爬虫(已有代码)
3. 🔧 添加14天数据清理功能(需要添加)

### 方案B: 生产级方案(1天) ⭐⭐ **长期使用推荐**

**适用场景**: 5-20人使用,需要稳定可靠

**优点**:
- ✅ 支持大量并发操作
- ✅ 数据安全有保障
- ✅ 自动备份

**缺点**:
- 💰 需要购买阿里云RDS(约30元/月起)
- ⏱️ 配置时间较长

**需要做的改动**:
1. 🔧 购买阿里云RDS数据库(MySQL或PostgreSQL)
2. 🔧 修改 `.env` 配置文件
3. 🔧 迁移数据(首次)
4. ✅ 部署定时爬虫(已有代码)
5. 🔧 添加14天数据清理功能

---

## 🛠️ 具体实施方案

### 方案A详细步骤

#### 步骤1: 部署现有系统(使用SQLite)

```bash
# 在本地执行
cd /Users/xiaoyan/Desktop/舆情/BettaFish-test
./一键部署百果园系统.sh
```

#### 步骤2: 添加数据清理功能

我需要创建一个新的脚本 `cleanup_old_data.py`:

```python
#!/usr/bin/env python3
"""
清理14天前的舆情数据
"""
import sqlite3
from datetime import datetime, timedelta

DB_PATH = "baiguoyuan_sentiment.db"
RETENTION_DAYS = 14  # 保留14天

def cleanup_old_data():
    """删除14天前的数据"""
    cutoff_date = datetime.now() - timedelta(days=RETENTION_DAYS)
    cutoff_str = cutoff_date.strftime('%Y-%m-%d')

    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    # 删除旧数据
    cursor.execute("""
        DELETE FROM sentiment_data
        WHERE created_at < ?
    """, (cutoff_str,))

    deleted = cursor.rowcount
    conn.commit()
    conn.close()

    print(f"✅ 清理完成: 删除了 {deleted} 条超过{RETENTION_DAYS}天的数据")
    return deleted

if __name__ == "__main__":
    cleanup_old_data()
```

#### 步骤3: 修改定时任务,添加每日清理

修改 `scheduler_daemon.py`,在第147行后添加:

```python
# 每天凌晨2点清理旧数据
schedule.every().day.at("02:00").do(cleanup_old_data_task)
```

#### 步骤4: 启动定时爬虫

在服务器上:
```bash
cd /root/BettaFish-test
source venv/bin/activate

# 后台启动定时爬虫
nohup python scheduler_daemon.py > scheduler.log 2>&1 &
echo $! > scheduler.pid
```

#### 步骤5: 配置systemd开机自启(可选)

创建 `/etc/systemd/system/baiguoyuan-scheduler.service`:

```ini
[Unit]
Description=百果园定时爬虫任务
After=network.target

[Service]
Type=simple
User=root
WorkingDirectory=/root/BettaFish-test
Environment="PATH=/root/BettaFish-test/venv/bin"
ExecStart=/root/BettaFish-test/venv/bin/python scheduler_daemon.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

启用:
```bash
systemctl daemon-reload
systemctl enable baiguoyuan-scheduler
systemctl start baiguoyuan-scheduler
```

---

### 方案B详细步骤

#### 步骤1: 购买阿里云RDS

**推荐配置** (适合小团队):
- 数据库类型: MySQL 8.0 或 PostgreSQL 14
- 规格: 1核1GB (约30元/月) 或 1核2GB (约60元/月)
- 存储: 20GB SSD
- 地域: **华北2(北京)** - 必须与ECS同地域

**购买流程**:
1. 登录 [阿里云RDS控制台](https://rdsnext.console.aliyun.com/)
2. 创建实例 → 选择MySQL/PostgreSQL → 选择配置
3. 网络配置: 选择VPC(与ECS相同的VPC)
4. 创建完成后,记录以下信息:
   - 内网地址: `rm-xxxxx.mysql.rds.aliyuncs.com`
   - 端口: `3306` (MySQL) 或 `5432` (PostgreSQL)

#### 步骤2: 配置数据库

```sql
-- MySQL示例
CREATE DATABASE baiguoyuan_sentiment
CHARACTER SET utf8mb4
COLLATE utf8mb4_unicode_ci;

-- 创建用户
CREATE USER 'baiguoyuan'@'%' IDENTIFIED BY 'YourStrongPassword123!';
GRANT ALL PRIVILEGES ON baiguoyuan_sentiment.* TO 'baiguoyuan'@'%';
FLUSH PRIVILEGES;
```

#### 步骤3: 配置白名单

在RDS控制台 → 数据安全性 → 白名单设置:
- 添加ECS内网IP,或
- 添加 `0.0.0.0/0` (仅测试用)

#### 步骤4: 更新服务器配置

修改 `/root/BettaFish-test/.env`:

```bash
# 数据库配置(使用云数据库)
DB_HOST=rm-xxxxx.mysql.rds.aliyuncs.com
DB_PORT=3306
DB_USER=baiguoyuan
DB_PASSWORD=YourStrongPassword123!
DB_NAME=baiguoyuan_sentiment
DB_CHARSET=utf8mb4
DB_DIALECT=mysql  # 或 postgresql
```

#### 步骤5: 初始化数据库表结构

在服务器上:
```bash
cd /root/BettaFish-test
source venv/bin/activate

# 安装MySQL客户端(如果用MySQL)
pip install pymysql asyncmy

# 或PostgreSQL客户端(如果用PostgreSQL)
pip install asyncpg psycopg2-binary

# 运行数据库初始化
cd MindSpider
python main.py --setup
```

#### 步骤6: 迁移现有数据(如果需要)

如果本地SQLite有测试数据:
```bash
# 在本地导出
sqlite3 baiguoyuan_sentiment.db .dump > data_backup.sql

# 转换为MySQL格式(手动调整)
# 或使用工具如 sqlite3-to-mysql

# 在服务器导入
mysql -h rm-xxxxx.mysql.rds.aliyuncs.com -u baiguoyuan -p baiguoyuan_sentiment < data_backup.sql
```

#### 步骤7: 重启所有服务

```bash
# 重启Streamlit界面
systemctl restart baiguoyuan

# 重启定时爬虫
systemctl restart baiguoyuan-scheduler
```

---

## 📊 方案对比总结

| 维度 | 方案A (SQLite) | 方案B (云数据库) |
|------|---------------|----------------|
| **部署时间** | 1小时 | 1天 |
| **成本** | 免费 | ~30-60元/月 |
| **并发用户** | 2-3人 | 20+人 |
| **稳定性** | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| **数据安全** | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| **扩展性** | ⭐⭐ | ⭐⭐⭐⭐⭐ |
| **运维难度** | 低 | 中 |

---

## 🎯 我的建议

### 阶段一(本周): 使用方案A快速上线

**理由**:
1. 你需要先验证系统是否满足业务需求
2. SQLite对2-3人使用完全够用
3. 省时省钱,快速迭代

**行动计划**:
- [x] 部署基础系统(用我提供的一键脚本)
- [ ] 添加14天数据清理功能(我帮你写)
- [ ] 启动定时爬虫
- [ ] 测试1-2周

### 阶段二(根据实际需要): 升级到方案B

**升级条件**(满足任一):
1. 使用人数 > 5人
2. 出现数据库锁定错误
3. 需要数据分析/报表功能

**升级优点**:
- 数据不丢失(可以迁移)
- 配置改动小(只需修改.env)
- 系统架构不变

---

## 📝 立即可用的完整部署方案

### 使用方案A(推荐先用)

#### 第1步: 创建数据清理脚本

我现在就帮你创建 `cleanup_old_data.py`

#### 第2步: 修改定时任务

我帮你更新 `scheduler_daemon.py`,添加数据清理

#### 第3步: 一键部署

```bash
cd /Users/xiaoyan/Desktop/舆情/BettaFish-test
./一键部署百果园系统.sh
```

#### 第4步: 启动定时爬虫

```bash
# SSH登录服务器
ssh root@101.201.214.42

# 启动定时爬虫
cd /root/BettaFish-test
source venv/bin/activate
nohup python scheduler_daemon.py > scheduler.log 2>&1 &
echo $! > scheduler.pid

# 检查是否启动成功
ps aux | grep scheduler_daemon.py
tail -f scheduler.log
```

#### 第5步: 分享给团队

发送访问链接: **http://101.201.214.42:5000**

任何人在浏览器打开此链接即可:
- 查看最新舆情数据
- 筛选和标记信息
- 手动触发数据刷新

---

## ❓ 常见问题

### Q1: 多人同时操作会不会有问题?

**A**: 方案A (SQLite):
- ✅ 2-3人同时查看: 没问题
- ⚠️ 2-3人同时标记: 偶尔会有1-2秒延迟
- ❌ 5人以上同时标记: 可能出现"数据库锁定"错误

**解决**: 如果频繁出错,升级到方案B(云数据库)

### Q2: 数据14天自动清理会丢失吗?

**A**:
- 建议在清理前自动备份数据库
- 我会在脚本中添加备份功能
- 备份文件保存在 `backups/` 目录

### Q3: 定时爬虫会不会占用太多资源?

**A**:
- 每次爬取约5-10分钟
- CPU占用 < 30%
- 内存占用 < 500MB
- 不影响Web界面使用

### Q4: 如何修改爬取时间?

**A**: 编辑 `scheduler_daemon.py`:
```python
# 第145-147行
schedule.every().day.at("08:00").do(morning_task)   # 改为 "09:00"
schedule.every().day.at("13:00").do(noon_task)      # 改为 "14:00"
schedule.every().day.at("20:00").do(evening_task)   # 改为 "21:00"
```

---

## 🚀 下一步行动

我现在就可以帮你:

1. **创建数据清理脚本** - 自动删除14天前数据
2. **更新定时任务代码** - 集成数据清理功能
3. **编写完整部署指令** - 一步步操作手册
4. **测试部署流程** - 确保一次成功

你希望我现在就开始吗?还是需要先了解更多细节?

---

**最后更新**: 2025-11-08
**适用版本**: 百果园舆情系统 v1.1
